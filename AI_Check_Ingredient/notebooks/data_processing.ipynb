{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e769b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cards written: novel_foods_cards.csv  (rows: 863)\n",
      "✅ Multi-vectors written: novel_foods_multivectors.csv  (rows: 1821)\n",
      "policy_item_id                    canonical                             entity_text\n",
      "        677319 3, 3'-Diindolylmethane (DIM) [CANON_EN] 3, 3'-Diindolylmethane (DIM)\n",
      "        677344         4-hydroxy isoleucine         [CANON_EN] 4-hydroxy isoleucine\n",
      "        677369          5-hydroxytryptophan          [CANON_EN] 5-hydroxytryptophan\n",
      "policy_item_id   section language                         text\n",
      "        677319  CANON_EN      UNK 3, 3'-Diindolylmethane (DIM)\n",
      "        677344  CANON_EN      UNK         4-hydroxy isoleucine\n",
      "        677369  CANON_EN      UNK          5-hydroxytryptophan\n",
      "        677449 CANON_LAT    LATIN               Abies balsamea\n",
      "        677449   SYN_LAT    LATIN   Abies balsamea f. balsamea\n",
      "        677479 CANON_LAT    LATIN              Abies pectinata\n"
     ]
    }
   ],
   "source": [
    "import json, re, pandas as pd\n",
    "from html import unescape\n",
    "from pathlib import Path\n",
    "\n",
    "# --------- Paths ---------\n",
    "input_path = Path(\"novel_foods_catalogue.json\")\n",
    "if not input_path.exists():\n",
    "    input_path = Path(\"novel_foods_catalogue.json\")\n",
    "\n",
    "cards_out = Path(\"novel_foods_cards.csv\")\n",
    "multiv_out = Path(\"novel_foods_multivectors.csv\")\n",
    "\n",
    "# --------- Utils ---------\n",
    "def strip_html(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r\"<[^>]*>\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def normalize_name(name):\n",
    "    return re.sub(r\"\\s+\", \" \", strip_html(name)).strip(\" ,;\")\n",
    "\n",
    "# split by commas/semicolons **outside parentheses**\n",
    "SPLIT_OUTSIDE_PAR = re.compile(r\"[;,]\\s*(?![^()]*\\))\")\n",
    "\n",
    "LATIN_BINOMIAL = re.compile(r\"^[A-Z][a-z]+(?:\\s+[a-z\\-]+){1,3}(?:\\s+[A-Za-z\\.\\(\\)]+)*$\")\n",
    "\n",
    "def is_latin_taxonomic(s: str) -> bool:\n",
    "    s = normalize_name(s)\n",
    "    return len(s) >= 4 and bool(LATIN_BINOMIAL.match(s))\n",
    "\n",
    "def parse_lang_labeled_block(raw: str):\n",
    "    \"\"\"\n",
    "    'EN: foo, FR: bar; DE: baz' -> {'EN': [...], 'FR': [...], 'DE': [...]}\n",
    "    (supports commas or semicolons outside parentheses)\n",
    "    \"\"\"\n",
    "    if not raw or not isinstance(raw, str):\n",
    "        return {}\n",
    "    raw = normalize_name(raw)\n",
    "    pat = re.compile(r\"([A-Z]{2})\\s*:\\s*([^:]+?)(?=(?:[A-Z]{2}\\s*:)|$)\")\n",
    "    out = {}\n",
    "    for m in pat.finditer(raw):\n",
    "        lang = m.group(1)\n",
    "        vals = [normalize_name(x) for x in SPLIT_OUTSIDE_PAR.split(m.group(2)) if x.strip()]\n",
    "        if vals:\n",
    "            out.setdefault(lang, []).extend(vals)\n",
    "    return out\n",
    "\n",
    "def parse_parenthetical_langs(raw: str):\n",
    "    \"\"\"\n",
    "    'Feijoa (DK, FI, FR)' or 'ginseng siberiano (ES) (PT)' -> {'DK':[Feijoa], 'FI':[Feijoa], ...}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not raw or not isinstance(raw, str):\n",
    "        return out\n",
    "    text = normalize_name(raw)\n",
    "    chunks = [c.strip() for c in SPLIT_OUTSIDE_PAR.split(text) if c.strip()]\n",
    "    for chunk in chunks:\n",
    "        groups = re.findall(r\"\\(([A-Z]{2}(?:\\s*,\\s*[A-Z]{2})*)\\)\", chunk)\n",
    "        name = re.sub(r\"\\s*\\(([A-Z]{2}(?:\\s*,\\s*[A-Z]{2})*)\\)\", \"\", chunk).strip()\n",
    "        name = normalize_name(name)\n",
    "        if not name:\n",
    "            continue\n",
    "        if groups:\n",
    "            codes = []\n",
    "            for g in groups:\n",
    "                codes.extend([c.strip() for c in g.split(\",\")])\n",
    "            for code in codes:\n",
    "                out.setdefault(code, []).append(name)\n",
    "        else:\n",
    "            out.setdefault(\"UNK\", []).append(name)\n",
    "    return out\n",
    "\n",
    "def parse_common_names(raw: str):\n",
    "    \"\"\"Tries 'EN:' style, then parenthetical codes. Fallback -> UNK split.\"\"\"\n",
    "    labeled = parse_lang_labeled_block(raw)\n",
    "    if labeled:\n",
    "        return labeled\n",
    "    paren = parse_parenthetical_langs(raw)\n",
    "    if paren:\n",
    "        return paren\n",
    "    if not raw:\n",
    "        return {}\n",
    "    names = [normalize_name(x) for x in SPLIT_OUTSIDE_PAR.split(str(raw)) if x.strip()]\n",
    "    return {\"UNK\": names} if names else {}\n",
    "\n",
    "def split_synonyms(text: str):\n",
    "    \"\"\"Synonyms split by commas/semicolons outside parentheses.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    text = normalize_name(text)\n",
    "    parts = [p for p in SPLIT_OUTSIDE_PAR.split(text) if p]\n",
    "    return parts\n",
    "\n",
    "# --------- Load JSON ---------\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Detect columns\n",
    "id_col     = next((c for c in df.columns if \"policy\" in c.lower() and \"id\" in c.lower()), None)\n",
    "canon_col  = next((c for c in df.columns if \"novel_food_name\" in c.lower()), None)\n",
    "common_col = next((c for c in df.columns if \"common\" in c.lower()), None)\n",
    "syn_col    = next((c for c in df.columns if \"synonym\" in c.lower()), None)\n",
    "assert id_col and canon_col, \"Missing required columns (policy_item_id & novel_food_name).\"\n",
    "\n",
    "# --------- Aggregate per entity ---------\n",
    "entities = {}\n",
    "for _, row in df.iterrows():\n",
    "    eid = str(row.get(id_col))\n",
    "    if not eid or eid == \"None\":\n",
    "        continue\n",
    "    canon = normalize_name(row.get(canon_col, \"\"))\n",
    "\n",
    "    common = row.get(common_col)\n",
    "    syn    = row.get(syn_col)\n",
    "\n",
    "    ent = entities.setdefault(eid, {\"canonical\": \"\", \"common\": {}, \"syn_lat\": [], \"syn_unk\": []})\n",
    "    if canon and not ent[\"canonical\"]:\n",
    "        ent[\"canonical\"] = canon  # first non-empty\n",
    "\n",
    "    # common names\n",
    "    if isinstance(common, str) and common.strip():\n",
    "        cmap = parse_common_names(common)\n",
    "        for lang, names in cmap.items():\n",
    "            bucket = ent[\"common\"].setdefault(lang, [])\n",
    "            for n in names:\n",
    "                n = normalize_name(n)\n",
    "                if n and n not in bucket:\n",
    "                    bucket.append(n)\n",
    "\n",
    "    # synonyms\n",
    "    if isinstance(syn, str) and syn.strip():\n",
    "        for s in split_synonyms(syn):\n",
    "            s = normalize_name(s)\n",
    "            if not s: \n",
    "                continue\n",
    "            if is_latin_taxonomic(s):\n",
    "                if s not in ent[\"syn_lat\"]:\n",
    "                    ent[\"syn_lat\"].append(s)\n",
    "            else:\n",
    "                if s not in ent[\"syn_unk\"]:\n",
    "                    ent[\"syn_unk\"].append(s)\n",
    "\n",
    "# --------- Build outputs ---------\n",
    "cards_rows = []\n",
    "multiv_rows = []  # one row per (entity, section, item)\n",
    "\n",
    "for eid, data in entities.items():\n",
    "    sections = []\n",
    "\n",
    "    # canonical\n",
    "    canon = data.get(\"canonical\", \"\")\n",
    "    if canon:\n",
    "        canon_tag = \"CANON_LAT\" if is_latin_taxonomic(canon) else \"CANON_EN\"\n",
    "        sections.append(f\"[{canon_tag}] {canon}\")\n",
    "        multiv_rows.append({\n",
    "            \"policy_item_id\": eid,\n",
    "            \"section\": canon_tag,\n",
    "            \"language\": \"LATIN\" if canon_tag==\"CANON_LAT\" else \"UNK\",\n",
    "            \"text\": canon\n",
    "        })\n",
    "\n",
    "    # common names by language\n",
    "    for lang in sorted(data[\"common\"].keys()):\n",
    "        names = sorted(set(data[\"common\"][lang]))\n",
    "        if names:\n",
    "            sections.append(f\"[COMMON_{lang}] \" + \" | \".join(names))\n",
    "            for n in names:\n",
    "                multiv_rows.append({\n",
    "                    \"policy_item_id\": eid,\n",
    "                    \"section\": f\"COMMON_{lang}\",\n",
    "                    \"language\": lang,\n",
    "                    \"text\": n\n",
    "                })\n",
    "\n",
    "    # synonyms (LATIN / UNK)\n",
    "    if data[\"syn_lat\"]:\n",
    "        syn_lat = sorted(set(data[\"syn_lat\"]))\n",
    "        sections.append(f\"[SYN_LAT] \" + \" [SEP] \".join(syn_lat))\n",
    "        for s in syn_lat:\n",
    "            multiv_rows.append({\n",
    "                \"policy_item_id\": eid,\n",
    "                \"section\": \"SYN_LAT\",\n",
    "                \"language\": \"LATIN\",\n",
    "                \"text\": s\n",
    "            })\n",
    "    if data[\"syn_unk\"]:\n",
    "        syn_unk = sorted(set(data[\"syn_unk\"]))\n",
    "        sections.append(f\"[SYN_UNK] \" + \" [SEP] \".join(syn_unk))\n",
    "        for s in syn_unk:\n",
    "            multiv_rows.append({\n",
    "                \"policy_item_id\": eid,\n",
    "                \"section\": \"SYN_UNK\",\n",
    "                \"language\": \"UNK\",\n",
    "                \"text\": s\n",
    "            })\n",
    "\n",
    "    cards_rows.append({\n",
    "        \"policy_item_id\": eid,\n",
    "        \"canonical\": canon,\n",
    "        \"entity_text\": \"\\n\".join(sections)\n",
    "    })\n",
    "\n",
    "# --------- Save ---------\n",
    "cards_df = pd.DataFrame(cards_rows)\n",
    "multiv_df = pd.DataFrame(multiv_rows)\n",
    "\n",
    "cards_df.to_csv(cards_out, index=False, encoding=\"utf-8\")\n",
    "multiv_df.to_csv(multiv_out, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Cards written: {cards_out}  (rows: {len(cards_df)})\")\n",
    "print(f\"✅ Multi-vectors written: {multiv_out}  (rows: {len(multiv_df)})\")\n",
    "\n",
    "# Optional peek\n",
    "display_cols_cards = [\"policy_item_id\",\"canonical\",\"entity_text\"]\n",
    "display_cols_mult  = [\"policy_item_id\",\"section\",\"language\",\"text\"]\n",
    "print(cards_df[display_cols_cards].head(3).to_string(index=False))\n",
    "print(multiv_df[display_cols_mult].head(6).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
