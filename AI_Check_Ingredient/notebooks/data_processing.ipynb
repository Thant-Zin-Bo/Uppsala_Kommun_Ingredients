{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7433d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "üîé Performing fuzzy merge for near-duplicate canonicals...\n",
      "‚úÖ Final entries: 1090\n",
      "üì¶ Saved:\n",
      " - cleaned_supplements_highacc.csv\n",
      " - cleaned_supplements_highacc.json\n",
      "                 canonical_name latin_name  \\\n",
      "0  3, 3'-Diindolylmethane (DIM)       None   \n",
      "1          4-hydroxy isoleucine       None   \n",
      "2                         5-HTP       None   \n",
      "3            5-Hydroxitryptofan       None   \n",
      "4           5-hydroxytryptophan       None   \n",
      "\n",
      "                                  variants   sources  \\\n",
      "0           [3, 3'-Diindolylmethane (DIM)]   [novel]   \n",
      "1                   [4-hydroxy isoleucine]   [novel]   \n",
      "2  [5-HTP, Oxitriptan, 5-Hydroxitryptofan]  [pharma]   \n",
      "3  [5-Hydroxitryptofan, Oxitriptan, 5-HTP]  [pharma]   \n",
      "4                    [5-hydroxytryptophan]   [novel]   \n",
      "\n",
      "           canonical_name_ascii latin_name_ascii  \n",
      "0  3, 3'-diindolylmethane (dim)             None  \n",
      "1          4-hydroxy isoleucine             None  \n",
      "2                         5-htp             None  \n",
      "3            5-hydroxitryptofan             None  \n",
      "4           5-hydroxytryptophan             None  \n"
     ]
    }
   ],
   "source": [
    "!pip install rapidfuzz\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# =========================================================\n",
    "# 1Ô∏è‚É£  Helpers\n",
    "# =========================================================\n",
    "\n",
    "def nfkc(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"Light cleanup with robust dash + punctuation handling.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = nfkc(s)\n",
    "    # unify dashes, invisible spaces\n",
    "    s = re.sub(r\"[\\u00A0\\u2000-\\u200B]\", \" \", s)\n",
    "    s = re.sub(r\"[\\u2212\\u2010-\\u2015]\", \"-\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = s.strip(\" ,;:\")\n",
    "    return s or None\n",
    "\n",
    "def norm_key(s):\n",
    "    \"\"\"Normalization for matching (lowercase + alnum only).\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    s = nfkc(s).lower()\n",
    "    s = re.sub(r\"[\\u2212\\u2010-\\u2015]\", \"-\", s)\n",
    "    s = re.sub(r\"[^a-z0-9 \\-]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or None\n",
    "\n",
    "def to_list(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return [x]\n",
    "    return []\n",
    "\n",
    "def unique_preserve_order(items):\n",
    "    seen, out = set(), []\n",
    "    for it in items:\n",
    "        if it is None:\n",
    "            continue\n",
    "        k = norm_key(it)\n",
    "        if not k or k in seen:\n",
    "            continue\n",
    "        seen.add(k)\n",
    "        out.append(it)\n",
    "    return out\n",
    "\n",
    "def ascii_fold(s):\n",
    "    \"\"\"Normalize accents and Greek letters to ASCII.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    # Greek Œ≤ ‚Üí beta, Œº ‚Üí micro etc.\n",
    "    greek_map = {\"Œ≤\": \"beta\", \"Œº\": \"micro\", \"Œ±\": \"alpha\", \"Œ≥\": \"gamma\"}\n",
    "    for g, r in greek_map.items():\n",
    "        s = s.replace(g, r)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s\n",
    "\n",
    "# Remove generic terms that pollute embeddings\n",
    "GENERIC_TERMS = [\"extract\", \"powder\", \"root\", \"leaf\", \"oil\", \"seed\", \"juice\"]\n",
    "def strip_generic_terms(s):\n",
    "    if not s: \n",
    "        return s\n",
    "    s = re.sub(r\"\\b(\" + \"|\".join(GENERIC_TERMS) + r\")\\b\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# =========================================================\n",
    "# 2Ô∏è‚É£  Load source JSONs\n",
    "# =========================================================\n",
    "\n",
    "with open(\"pharmaceutical_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pharma = json.load(f)\n",
    "\n",
    "with open(\"novel_foods_catalogue.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    novel = json.load(f)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# ---------------- Pharmaceutical ----------------\n",
    "for rec in pharma:\n",
    "    canonical = clean_text(rec.get(\"name\"))\n",
    "    if not canonical:\n",
    "        continue\n",
    "    variants = [canonical] + [clean_text(x) for x in to_list(rec.get(\"synonyms\"))]\n",
    "    variants = [strip_generic_terms(v) for v in variants if v]\n",
    "    variants = unique_preserve_order(variants)\n",
    "    rows.append({\n",
    "        \"canonical_name\": strip_generic_terms(canonical),\n",
    "        \"latin_name\": None,\n",
    "        \"variants\": variants,\n",
    "        \"sources\": [\"pharma\"]\n",
    "    })\n",
    "\n",
    "# ---------------- Novel Foods ----------------\n",
    "for rec in novel:\n",
    "    canonical = clean_text(rec.get(\"novel_food_name\"))\n",
    "    if not canonical:\n",
    "        continue\n",
    "    latin_name = clean_text(rec.get(\"latin_name\")) if \"latin_name\" in rec else None\n",
    "    common_name = clean_text(rec.get(\"common_name\"))\n",
    "    syns = [clean_text(x) for x in to_list(rec.get(\"synonyms\"))]\n",
    "\n",
    "    variants = [canonical]\n",
    "    if common_name: variants.append(common_name)\n",
    "    if latin_name: variants.append(latin_name)\n",
    "    variants.extend(syns)\n",
    "    variants = [strip_generic_terms(v) for v in variants if v]\n",
    "    variants = unique_preserve_order(variants)\n",
    "\n",
    "    rows.append({\n",
    "        \"canonical_name\": strip_generic_terms(canonical),\n",
    "        \"latin_name\": strip_generic_terms(latin_name),\n",
    "        \"variants\": variants,\n",
    "        \"sources\": [\"novel\"]\n",
    "    })\n",
    "\n",
    "# =========================================================\n",
    "# 3Ô∏è‚É£  Merge duplicates by canonical\n",
    "# =========================================================\n",
    "merged = {}\n",
    "for r in rows:\n",
    "    key = norm_key(r[\"canonical_name\"])\n",
    "    if key not in merged:\n",
    "        merged[key] = r\n",
    "    else:\n",
    "        m = merged[key]\n",
    "        if (not m[\"latin_name\"]) and r[\"latin_name\"]:\n",
    "            m[\"latin_name\"] = r[\"latin_name\"]\n",
    "        m[\"variants\"] = unique_preserve_order(m[\"variants\"] + r[\"variants\"])\n",
    "        m[\"sources\"] = unique_preserve_order(m[\"sources\"] + r[\"sources\"])\n",
    "\n",
    "cleaned = list(merged.values())\n",
    "\n",
    "# =========================================================\n",
    "# 4Ô∏è‚É£  Fuzzy merge near-duplicates (‚â•95% lexical similarity)\n",
    "# =========================================================\n",
    "print(\"üîé Performing fuzzy merge for near-duplicate canonicals...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "used = set()\n",
    "merged_rows = []\n",
    "for i, row in df.iterrows():\n",
    "    if i in used: \n",
    "        continue\n",
    "    similar = df[df[\"canonical_name\"].apply(lambda x: fuzz.token_sort_ratio(x, row[\"canonical_name\"]) >= 95)]\n",
    "    used.update(similar.index)\n",
    "    all_variants = sum(similar[\"variants\"].tolist(), [])\n",
    "    merged_rows.append({\n",
    "        \"canonical_name\": row[\"canonical_name\"],\n",
    "        \"latin_name\": row[\"latin_name\"] or next((x for x in similar[\"latin_name\"] if x), None),\n",
    "        \"variants\": unique_preserve_order(all_variants),\n",
    "        \"sources\": unique_preserve_order(sum(similar[\"sources\"].tolist(), []))\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(merged_rows)\n",
    "\n",
    "# =========================================================\n",
    "# 5Ô∏è‚É£  Accent & Greek normalization for stability\n",
    "# =========================================================\n",
    "df[\"canonical_name_ascii\"] = df[\"canonical_name\"].apply(ascii_fold).str.lower()\n",
    "df[\"latin_name_ascii\"] = df[\"latin_name\"].apply(ascii_fold).str.lower()\n",
    "\n",
    "# =========================================================\n",
    "# 6Ô∏è‚É£  Save outputs\n",
    "# =========================================================\n",
    "df = df.sort_values(\"canonical_name\").reset_index(drop=True)\n",
    "df.to_csv(\"cleaned_supplements_highacc.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "with open(\"cleaned_supplements_highacc.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(df.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Final entries: {len(df)}\")\n",
    "print(\"üì¶ Saved:\")\n",
    "print(\" - cleaned_supplements_highacc.csv\")\n",
    "print(\" - cleaned_supplements_highacc.json\")\n",
    "print(df.head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
