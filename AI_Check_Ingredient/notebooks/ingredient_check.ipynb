{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae678121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Ingredient Search API...\n",
      "‚öôÔ∏è Loading model: sentence-transformers/distiluse-base-multilingual-cased-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Encoding cards texts‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [01:17<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS (cards) built & saved.\n",
      "‚öôÔ∏è Loading model: sentence-transformers/distiluse-base-multilingual-cased-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS (multivector) loaded.\n",
      "‚úÖ Both indices ready.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://172.17.0.2:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [08/Oct/2025 13:09:31] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Ingredient Search API ‚Äî Single Best Match (Cards & Multi-Vector)\n",
    "# ===============================================================\n",
    "\n",
    "import os, json, re, time, unicodedata, numpy as np, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rapidfuzz import fuzz\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "# Optional FAISS acceleration\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CONFIGURATION\n",
    "# ===============================================================\n",
    "DEFAULT_MODE = \"multivector\"  # \"cards\" or \"multivector\"\n",
    "\n",
    "# Input files\n",
    "CARDS_PATH = \"novel_foods_cards.csv\"\n",
    "MULTIV_PATH = \"novel_foods_multivectors.csv\"\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "\n",
    "# Cache folders\n",
    "CACHE_DIR = \"indices_v2\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Cards cache\n",
    "EMB_CARDS = os.path.join(CACHE_DIR, \"emb_cards.npy\")\n",
    "IDX_CARDS = os.path.join(CACHE_DIR, \"index_cards.faiss\")\n",
    "LOOKUP_CARDS = os.path.join(CACHE_DIR, \"lookup_cards.csv\")\n",
    "META_CARDS = os.path.join(CACHE_DIR, \"meta_cards.json\")\n",
    "\n",
    "# Multivector cache\n",
    "EMB_MULTI = os.path.join(CACHE_DIR, \"emb_multi.npy\")\n",
    "IDX_MULTI = os.path.join(CACHE_DIR, \"index_multi.faiss\")\n",
    "LOOKUP_MULTI = os.path.join(CACHE_DIR, \"lookup_multi.csv\")\n",
    "META_MULTI = os.path.join(CACHE_DIR, \"meta_multi.json\")\n",
    "\n",
    "# Search settings\n",
    "RECALL_K = 200\n",
    "ALPHA_SEM = 0.75\n",
    "MIN_CONFIDENCE = 0.50\n",
    "\n",
    "# Section weighting\n",
    "SECTION_BOOST = {\n",
    "    \"CANON_LAT\": 1.05,\n",
    "    \"CANON_EN\": 1.00,\n",
    "    \"SYN_LAT\": 0.95,\n",
    "}\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# HELPERS\n",
    "# ===============================================================\n",
    "def normalize_query_lex(s: str) -> str:\n",
    "    \"\"\"Normalize query text for lexical comparison.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", s.casefold())\n",
    "    s = re.sub(r\"[\\u2212\\u2010-\\u2015]\", \"-\", s)\n",
    "    s = re.sub(r\"[^a-z0-9 \\-\\u00C0-\\u017F]\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def ensure_cols(df, cols):\n",
    "    \"\"\"Ensure all required columns exist.\"\"\"\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "def section_boost(section: str) -> float:\n",
    "    \"\"\"Get weighting multiplier for a section.\"\"\"\n",
    "    if section in SECTION_BOOST:\n",
    "        return SECTION_BOOST[section]\n",
    "    if section.startswith(\"COMMON_\"):\n",
    "        return 1.00\n",
    "    return 1.00\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# LOAD DATA & BUILD INDEX\n",
    "# ===============================================================\n",
    "def load_cards():\n",
    "    df = pd.read_csv(CARDS_PATH, dtype={\"policy_item_id\": str})\n",
    "    ensure_cols(df, [\"policy_item_id\", \"canonical\", \"entity_text\"])\n",
    "    lookup = df[[\"policy_item_id\", \"canonical\", \"entity_text\"]].copy()\n",
    "    return df, lookup\n",
    "\n",
    "\n",
    "def load_multivectors():\n",
    "    mv = pd.read_csv(MULTIV_PATH, dtype={\"policy_item_id\": str})\n",
    "    ensure_cols(mv, [\"policy_item_id\", \"section\", \"language\", \"text\"])\n",
    "    if os.path.exists(CARDS_PATH):\n",
    "        cards = pd.read_csv(CARDS_PATH, dtype={\"policy_item_id\": str})\n",
    "        can_map = cards[[\"policy_item_id\", \"canonical\"]].drop_duplicates()\n",
    "    else:\n",
    "        can_map = pd.DataFrame(columns=[\"policy_item_id\", \"canonical\"])\n",
    "    lookup = mv.merge(can_map, on=\"policy_item_id\", how=\"left\")\n",
    "    return mv, lookup\n",
    "\n",
    "\n",
    "def build_index(mode=\"multivector\"):\n",
    "    \"\"\"Build or load embeddings & FAISS index for given mode.\"\"\"\n",
    "    print(f\"‚öôÔ∏è Loading model: {MODEL_NAME}\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    if mode == \"cards\":\n",
    "        df, lookup = load_cards()\n",
    "        texts = df[\"entity_text\"].astype(str).tolist()\n",
    "        emb_path, idx_path, meta_path, lookup_path = EMB_CARDS, IDX_CARDS, META_CARDS, LOOKUP_CARDS\n",
    "    else:\n",
    "        df, lookup = load_multivectors()\n",
    "        texts = df[\"text\"].astype(str).tolist()\n",
    "        emb_path, idx_path, meta_path, lookup_path = EMB_MULTI, IDX_MULTI, META_MULTI, LOOKUP_MULTI\n",
    "\n",
    "    if os.path.exists(emb_path) and os.path.exists(meta_path):\n",
    "        try:\n",
    "            meta = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "            if meta.get(\"row_count\") == len(texts) and meta.get(\"model\") == MODEL_NAME:\n",
    "                emb = np.load(emb_path)\n",
    "                df_lookup = pd.read_csv(lookup_path, dtype={\"policy_item_id\": str})\n",
    "                index = None\n",
    "                if FAISS_AVAILABLE and os.path.exists(idx_path):\n",
    "                    index = faiss.read_index(idx_path)\n",
    "                    print(f\"‚úÖ FAISS ({mode}) loaded.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è FAISS not installed ‚Äî cosine fallback ({mode}).\")\n",
    "                return model, emb, index, df_lookup\n",
    "        except Exception:\n",
    "            print(f\"‚ôªÔ∏è Cache mismatch ‚Äî rebuilding {mode} index.\")\n",
    "\n",
    "    print(f\"‚öôÔ∏è Encoding {mode} texts‚Ä¶\")\n",
    "    emb = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n",
    "    np.save(emb_path, emb)\n",
    "    lookup.to_csv(lookup_path, index=False)\n",
    "    json.dump({\"model\": MODEL_NAME, \"row_count\": len(texts)}, open(meta_path, \"w\"))\n",
    "\n",
    "    index = None\n",
    "    if FAISS_AVAILABLE:\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        index.add(np.array(emb, dtype=\"float32\"))\n",
    "        faiss.write_index(index, idx_path)\n",
    "        print(f\"‚úÖ FAISS ({mode}) built & saved.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è FAISS not available ‚Äî using cosine similarity ({mode}).\")\n",
    "\n",
    "    return model, emb, index, lookup\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# SEARCH\n",
    "# ===============================================================\n",
    "def search_best(query, model, emb, index, df_lookup, mode=\"multivector\"):\n",
    "    \"\"\"Return single best match (highest blended score).\"\"\"\n",
    "    q_emb = model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    if FAISS_AVAILABLE and index is not None:\n",
    "        scores, idx = index.search(np.array(q_emb, dtype=\"float32\"), min(RECALL_K, len(df_lookup)))\n",
    "        idx, scores = idx[0], scores[0]\n",
    "    else:\n",
    "        sims = cosine_similarity(q_emb, emb)[0]\n",
    "        idx = np.argsort(sims)[::-1][:min(RECALL_K, len(df_lookup))]\n",
    "        scores = sims[idx]\n",
    "\n",
    "    q_norm = normalize_query_lex(query)\n",
    "    best = None\n",
    "\n",
    "    for i, s in zip(idx, scores):\n",
    "        row = df_lookup.iloc[i]\n",
    "        if mode == \"cards\":\n",
    "            text = str(row.get(\"entity_text\", \"\"))\n",
    "            section = \"CARD\"\n",
    "            lang = \"\"\n",
    "        else:\n",
    "            text = str(row.get(\"text\", \"\"))\n",
    "            section = str(row.get(\"section\", \"\"))\n",
    "            lang = str(row.get(\"language\", \"\"))\n",
    "\n",
    "        canon = str(row.get(\"canonical\", \"\"))\n",
    "        lex1 = fuzz.token_set_ratio(q_norm, normalize_query_lex(text)) / 100\n",
    "        lex2 = fuzz.token_set_ratio(q_norm, normalize_query_lex(canon)) / 100 if canon else 0\n",
    "        lex = max(lex1, lex2)\n",
    "\n",
    "        boosted = float(s) * section_boost(section)\n",
    "        final = ALPHA_SEM * boosted + (1 - ALPHA_SEM) * lex\n",
    "        if final < MIN_CONFIDENCE:\n",
    "            continue\n",
    "\n",
    "        if (best is None) or (final > best[\"score\"]):\n",
    "            best = {\n",
    "                \"policy_item_id\": str(row[\"policy_item_id\"]),\n",
    "                \"canonical\": canon,\n",
    "                \"best_text\": text,\n",
    "                \"section\": section,\n",
    "                \"language\": lang,\n",
    "                \"semantic\": round(float(s), 3),\n",
    "                \"lexical\": round(lex, 3),\n",
    "                \"score\": round(final, 3),\n",
    "            }\n",
    "\n",
    "    return best or {}\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# FLASK SERVER\n",
    "# ===============================================================\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "print(\"üöÄ Starting Ingredient Search API...\")\n",
    "model_cards, emb_cards, index_cards, df_cards = build_index(\"cards\")\n",
    "model_multi, emb_multi, index_multi, df_multi = build_index(\"multivector\")\n",
    "print(\"‚úÖ Both indices ready.\")\n",
    "\n",
    "\n",
    "@app.route(\"/search\", methods=[\"POST\"])\n",
    "def search_api():\n",
    "    \"\"\"\n",
    "    POST {query: 'vitamin a', mode: 'cards'|'multivector'}\n",
    "    Returns the single best match as JSON.\n",
    "    \"\"\"\n",
    "    data = request.get_json(force=True)\n",
    "    query = data.get(\"query\", \"\")\n",
    "    mode = data.get(\"mode\", DEFAULT_MODE).lower()\n",
    "\n",
    "    if not query:\n",
    "        return jsonify({\"error\": \"Missing 'query'\"}), 400\n",
    "    if mode not in [\"cards\", \"multivector\"]:\n",
    "        mode = DEFAULT_MODE\n",
    "\n",
    "    if mode == \"cards\":\n",
    "        best = search_best(query, model_cards, emb_cards, index_cards, df_cards, mode=\"cards\")\n",
    "    else:\n",
    "        best = search_best(query, model_multi, emb_multi, index_multi, df_multi, mode=\"multivector\")\n",
    "\n",
    "    return jsonify(best)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5001, debug=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
