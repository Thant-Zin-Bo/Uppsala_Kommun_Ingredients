{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae678121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Ingredient Search ‚Äî Mode: multivector\n",
      "‚öôÔ∏è Loading model: sentence-transformers/distiluse-base-multilingual-cased-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Encoding multivector texts‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [01:23<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Embedded 1821 rows in 83.8s\n",
      "‚úÖ FAISS (multivector) built & saved.\n",
      "\n",
      "‚úÖ Ready. Type any ingredient name (or 'exit' to quit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 'vit A':\n",
      "======================================================================\n",
      "Entity: Betaine (ID: 1673130)\n",
      "  Best match text: N\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.747, lexical: 0.333, final: 0.643\n",
      "----------------------------------------------------------------------\n",
      "Entity: Eleocharis dulcis (ID: 685187)\n",
      "  Best match text: E. indica\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.742, lexical: 0.308, final: 0.633\n",
      "----------------------------------------------------------------------\n",
      "Entity: Ligusticum striatum (ID: 1948733)\n",
      "  Best match text: K.Y.Pan\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.609, lexical: 0.333, final: 0.54\n",
      "----------------------------------------------------------------------\n",
      "Entity: Bambusa spp. (ID: 702102)\n",
      "  Best match text: ŒúœÄŒ±ŒºœÄŒøœç (EL)\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.607, lexical: 0.25, final: 0.518\n",
      "----------------------------------------------------------------------\n",
      "Entity: Acer nigrum (ID: 677721)\n",
      "  Best match text: Acer nigrum\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.538, lexical: 0.375, final: 0.517\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'Vitamin':\n",
      "======================================================================\n",
      "Entity: Liposomal vitamin C (ID: 689734)\n",
      "  Best match text: Liposomal vitamin C\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.714, lexical: 1.0, final: 0.812\n",
      "----------------------------------------------------------------------\n",
      "Entity: Betaine (ID: 1673130)\n",
      "  Best match text: Betaine\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.544, lexical: 0.571, final: 0.551\n",
      "----------------------------------------------------------------------\n",
      "Entity: Vitis vinifera (ID: 1917494)\n",
      "  Best match text: Vitis vinifera\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.547, lexical: 0.381, final: 0.526\n",
      "----------------------------------------------------------------------\n",
      "Entity: Serratiopeptidase (ID: 696211)\n",
      "  Best match text: serrapeptidase\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.604, lexical: 0.25, final: 0.516\n",
      "----------------------------------------------------------------------\n",
      "Entity: Bovine lactoferrin (ID: 681299)\n",
      "  Best match text: Bovine lactoferrin\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.525, lexical: 0.4, final: 0.514\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'Acacia':\n",
      "======================================================================\n",
      "Entity: Acacia verek (ID: 677638)\n",
      "  Best match text: Acacia verek\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.902, lexical: 1.0, final: 0.96\n",
      "----------------------------------------------------------------------\n",
      "Entity: Acacia arabica (ID: 677504)\n",
      "  Best match text: Acacia arabica\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.87, lexical: 1.0, final: 0.935\n",
      "----------------------------------------------------------------------\n",
      "Entity: Acacia senegal (ID: 700382)\n",
      "  Best match text: Acacia senegal\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.822, lexical: 1.0, final: 0.897\n",
      "----------------------------------------------------------------------\n",
      "Entity: Acacia nilotica (ID: 677529)\n",
      "  Best match text: Acacia nilotica\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.8, lexical: 1.0, final: 0.88\n",
      "----------------------------------------------------------------------\n",
      "Entity: Acacia rigidula (ID: 677558)\n",
      "  Best match text: Acacia rigidula\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.74, lexical: 1.0, final: 0.833\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'senticosus':\n",
      "======================================================================\n",
      "Entity: Acanthopanax senticosus (ID: 677663)\n",
      "  Best match text: Acanthopanax senticosus\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.719, lexical: 1.0, final: 0.816\n",
      "----------------------------------------------------------------------\n",
      "Entity: Stachyose (ID: 697056)\n",
      "  Best match text: Stachyose\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.787, lexical: 0.526, final: 0.722\n",
      "----------------------------------------------------------------------\n",
      "Entity: Serratiopeptidase (ID: 696211)\n",
      "  Best match text: serratiapeptase\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.798, lexical: 0.444, final: 0.71\n",
      "----------------------------------------------------------------------\n",
      "Entity: Curcuminoids (ID: 684245)\n",
      "  Best match text: Curcuminoids\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.831, lexical: 0.273, final: 0.691\n",
      "----------------------------------------------------------------------\n",
      "Entity: Splenopentin (ID: 696939)\n",
      "  Best match text: Splenopentin\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.766, lexical: 0.455, final: 0.688\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'ovinu':\n",
      "======================================================================\n",
      "Entity: Curcumin (ID: 684220)\n",
      "  Best match text: Curcumin\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.739, lexical: 0.308, final: 0.631\n",
      "----------------------------------------------------------------------\n",
      "Entity: Zeolite (ID: 699303)\n",
      "  Best match text: Zeolite\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.721, lexical: 0.333, final: 0.624\n",
      "----------------------------------------------------------------------\n",
      "Entity: Serratiopeptidase (ID: 696211)\n",
      "  Best match text: serralysin\n",
      "  Section/Lang: SYN_UNK / UNK\n",
      "  Scores ‚Üí semantic: 0.739, lexical: 0.267, final: 0.621\n",
      "----------------------------------------------------------------------\n",
      "Entity: Rutin (ID: 695707)\n",
      "  Best match text: Rutin\n",
      "  Section/Lang: CANON_EN / UNK\n",
      "  Scores ‚Üí semantic: 0.681, lexical: 0.4, final: 0.611\n",
      "----------------------------------------------------------------------\n",
      "Entity: Albatrellus ovinus (ID: 678346)\n",
      "  Best match text: Albatrellus ovinus\n",
      "  Section/Lang: CANON_LAT / LATIN\n",
      "  Scores ‚Üí semantic: 0.623, lexical: 0.435, final: 0.599\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Ingredient Search ‚Äî Cards & Multi-Vector Index\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Optional FAISS acceleration\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "# Choose: \"cards\" (one vector per entity) or \"multivector\" (many vectors per entity)\n",
    "MODE = \"multivector\"   # \"cards\" or \"multivector\"\n",
    "\n",
    "# Input data produced by the preprocessing you ran earlier\n",
    "CARDS_PATH    = \"novel_foods_cards.csv\"           # columns: policy_item_id, canonical, entity_text\n",
    "MULTIV_PATH   = \"novel_foods_multivectors.csv\"    # columns: policy_item_id, section, language, text\n",
    "\n",
    "# Model (multilingual)\n",
    "MODEL_NAME    = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "# Alternatives:\n",
    "# MODEL_NAME  = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# MODEL_NAME  = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# Embeddings / index cache\n",
    "CACHE_DIR     = \"indices_v2\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "EMB_CARDS     = os.path.join(CACHE_DIR, \"emb_cards.npy\")\n",
    "IDX_CARDS     = os.path.join(CACHE_DIR, \"index_cards.faiss\")\n",
    "LOOKUP_CARDS  = os.path.join(CACHE_DIR, \"lookup_cards.csv\")\n",
    "META_CARDS    = os.path.join(CACHE_DIR, \"meta_cards.json\")\n",
    "\n",
    "EMB_MULTI     = os.path.join(CACHE_DIR, \"emb_multi.npy\")\n",
    "IDX_MULTI     = os.path.join(CACHE_DIR, \"index_multi.faiss\")\n",
    "LOOKUP_MULTI  = os.path.join(CACHE_DIR, \"lookup_multi.csv\")\n",
    "META_MULTI    = os.path.join(CACHE_DIR, \"meta_multi.json\")\n",
    "\n",
    "# Search hyperparams\n",
    "TOP_K_DEFAULT = 5        # final results to return\n",
    "RECALL_K      = 200      # how many candidates to pull from ANN/sims before aggregation\n",
    "ALPHA_SEM     = 0.75     # semantic vs. lexical blend\n",
    "MIN_CONFIDENCE= 0.50     # minimum blended score to keep\n",
    "\n",
    "# Optional: section boosts for multivector aggregation\n",
    "SECTION_BOOST = {\n",
    "    \"CANON_LAT\": 1.05,\n",
    "    \"CANON_EN\":  1.00,\n",
    "    \"SYN_LAT\":   0.95,\n",
    "    # COMMON_XX will default to 1.00; you can add specific boosts like \"COMMON_DE\": 1.02, etc.\n",
    "}\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Helpers\n",
    "# ===============================================================\n",
    "def normalize_query_lex(s: str) -> str:\n",
    "    \"\"\"Light normalization for fuzzy matching (lexical). Keep diacritics? We fold to be robust.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", s.casefold())\n",
    "    s = re.sub(r\"[\\u2212\\u2010-\\u2015]\", \"-\", s)\n",
    "    s = re.sub(r\"[^a-z0-9 \\-\\u00C0-\\u017F]\", \" \", s)  # keep basic Latin-1 letters\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Data loaders\n",
    "# ===============================================================\n",
    "def load_cards():\n",
    "    df = pd.read_csv(CARDS_PATH, dtype={\"policy_item_id\": str})\n",
    "    ensure_cols(df, [\"policy_item_id\", \"canonical\", \"entity_text\"])\n",
    "    # Lookup used when returning results\n",
    "    lookup = df[[\"policy_item_id\", \"canonical\", \"entity_text\"]].copy()\n",
    "    return df, lookup\n",
    "\n",
    "def load_multivectors():\n",
    "    mv = pd.read_csv(MULTIV_PATH, dtype={\"policy_item_id\": str})\n",
    "    ensure_cols(mv, [\"policy_item_id\", \"section\", \"language\", \"text\"])\n",
    "    # We also want a canonical name map for pretty printing\n",
    "    if os.path.exists(CARDS_PATH):\n",
    "        cards = pd.read_csv(CARDS_PATH, dtype={\"policy_item_id\": str})\n",
    "        can_map = cards[[\"policy_item_id\", \"canonical\"]].drop_duplicates()\n",
    "    else:\n",
    "        can_map = pd.DataFrame(columns=[\"policy_item_id\",\"canonical\"])\n",
    "    lookup = mv.merge(can_map, on=\"policy_item_id\", how=\"left\")\n",
    "    return mv, lookup\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Build or load indices (cards)\n",
    "# ===============================================================\n",
    "def build_or_load_cards_index():\n",
    "    print(\"‚öôÔ∏è Loading model:\", MODEL_NAME)\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    df, lookup = load_cards()\n",
    "    texts = df[\"entity_text\"].astype(str).tolist()\n",
    "\n",
    "    use_cache = (\n",
    "        os.path.exists(EMB_CARDS) and os.path.exists(LOOKUP_CARDS) and os.path.exists(META_CARDS)\n",
    "    )\n",
    "    if use_cache:\n",
    "        try:\n",
    "            with open(META_CARDS, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            if meta.get(\"row_count\") == len(texts) and meta.get(\"model\") == MODEL_NAME:\n",
    "                print(\"üîÅ Loading cached card embeddings‚Ä¶\")\n",
    "                emb = np.load(EMB_CARDS)\n",
    "                df_lookup = pd.read_csv(LOOKUP_CARDS, dtype={\"policy_item_id\": str})\n",
    "                index = None\n",
    "                if FAISS_AVAILABLE and os.path.exists(IDX_CARDS):\n",
    "                    index = faiss.read_index(IDX_CARDS)\n",
    "                    print(\"‚úÖ FAISS (cards) loaded.\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è FAISS not available; using cosine similarity for cards.\")\n",
    "                return model, emb, index, df_lookup\n",
    "        except Exception:\n",
    "            print(\"‚ôªÔ∏è Cache mismatch; rebuilding card index.\")\n",
    "\n",
    "    print(\"‚öôÔ∏è Encoding card texts‚Ä¶\")\n",
    "    t0 = time.time()\n",
    "    emb = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n",
    "    print(f\"‚è±Ô∏è Embedded {len(texts)} cards in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    np.save(EMB_CARDS, emb)\n",
    "    lookup.to_csv(LOOKUP_CARDS, index=False)\n",
    "    with open(META_CARDS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"model\": MODEL_NAME, \"row_count\": len(texts)}, f)\n",
    "\n",
    "    if FAISS_AVAILABLE:\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        index.add(np.array(emb, dtype=\"float32\"))\n",
    "        faiss.write_index(index, IDX_CARDS)\n",
    "        print(\"‚úÖ FAISS (cards) built & saved.\")\n",
    "    else:\n",
    "        index = None\n",
    "        print(\"‚ö†Ô∏è FAISS not installed ‚Äî using cosine similarity at query time (cards).\")\n",
    "\n",
    "    return model, emb, index, lookup\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Build or load indices (multi-vector)\n",
    "# ===============================================================\n",
    "def build_or_load_multivector_index():\n",
    "    print(\"‚öôÔ∏è Loading model:\", MODEL_NAME)\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    mv, lookup = load_multivectors()\n",
    "    texts = mv[\"text\"].astype(str).tolist()\n",
    "\n",
    "    use_cache = (\n",
    "        os.path.exists(EMB_MULTI) and os.path.exists(LOOKUP_MULTI) and os.path.exists(META_MULTI)\n",
    "    )\n",
    "    if use_cache:\n",
    "        try:\n",
    "            with open(META_MULTI, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            if meta.get(\"row_count\") == len(texts) and meta.get(\"model\") == MODEL_NAME:\n",
    "                print(\"üîÅ Loading cached multivector embeddings‚Ä¶\")\n",
    "                emb = np.load(EMB_MULTI)\n",
    "                df_lookup = pd.read_csv(LOOKUP_MULTI, dtype={\"policy_item_id\": str})\n",
    "                index = None\n",
    "                if FAISS_AVAILABLE and os.path.exists(IDX_MULTI):\n",
    "                    index = faiss.read_index(IDX_MULTI)\n",
    "                    print(\"‚úÖ FAISS (multivector) loaded.\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è FAISS not available; using cosine similarity for multivector.\")\n",
    "                return model, emb, index, df_lookup\n",
    "        except Exception:\n",
    "            print(\"‚ôªÔ∏è Cache mismatch; rebuilding multivector index.\")\n",
    "\n",
    "    print(\"‚öôÔ∏è Encoding multivector texts‚Ä¶\")\n",
    "    t0 = time.time()\n",
    "    emb = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n",
    "    print(f\"‚è±Ô∏è Embedded {len(texts)} rows in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    np.save(EMB_MULTI, emb)\n",
    "    lookup.to_csv(LOOKUP_MULTI, index=False)\n",
    "    with open(META_MULTI, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"model\": MODEL_NAME, \"row_count\": len(texts)}, f)\n",
    "\n",
    "    if FAISS_AVAILABLE:\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        index.add(np.array(emb, dtype=\"float32\"))\n",
    "        faiss.write_index(index, IDX_MULTI)\n",
    "        print(\"‚úÖ FAISS (multivector) built & saved.\")\n",
    "    else:\n",
    "        index = None\n",
    "        print(\"‚ö†Ô∏è FAISS not installed ‚Äî using cosine similarity at query time (multivector).\")\n",
    "\n",
    "    return model, emb, index, lookup\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Search (cards)\n",
    "# ===============================================================\n",
    "def search_cards(query, model, emb, index, df_lookup, top_k=TOP_K_DEFAULT):\n",
    "    q_emb = model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    # ANN / sims\n",
    "    if FAISS_AVAILABLE and index is not None:\n",
    "        scores, idx = index.search(np.array(q_emb, dtype=\"float32\"), min(RECALL_K, len(df_lookup)))\n",
    "        idx, scores = idx[0], scores[0]\n",
    "    else:\n",
    "        sims = cosine_similarity(q_emb, emb)[0]\n",
    "        idx = np.argsort(sims)[::-1][:min(RECALL_K, len(df_lookup))]\n",
    "        scores = sims[idx]\n",
    "\n",
    "    # Blend with lexical on canonical + entity_text\n",
    "    results = []\n",
    "    q_norm = normalize_query_lex(query)\n",
    "    for i, s in zip(idx, scores):\n",
    "        row = df_lookup.iloc[i]\n",
    "        canon = str(row.get(\"canonical\", \"\"))\n",
    "        blob  = str(row.get(\"entity_text\", \"\"))\n",
    "\n",
    "        # lexical score against canonical and a shorter slice of the blob (avoid huge text bias)\n",
    "        lex1 = fuzz.token_set_ratio(q_norm, normalize_query_lex(canon)) / 100\n",
    "        lex2 = fuzz.partial_ratio(q_norm, normalize_query_lex(blob[:500])) / 100\n",
    "        lex  = max(lex1, lex2)\n",
    "\n",
    "        final = ALPHA_SEM * float(s) + (1 - ALPHA_SEM) * lex\n",
    "        if final >= MIN_CONFIDENCE:\n",
    "            results.append({\n",
    "                \"policy_item_id\": str(row[\"policy_item_id\"]),\n",
    "                \"canonical\": canon,\n",
    "                \"best_text\": canon,\n",
    "                \"section\": \"CARD\",\n",
    "                \"language\": \"\",\n",
    "                \"semantic\": round(float(s), 3),\n",
    "                \"lexical\": round(lex, 3),\n",
    "                \"score\": round(final, 3),\n",
    "            })\n",
    "\n",
    "    # De-dup by entity and keep best\n",
    "    best_by_ent = {}\n",
    "    for r in results:\n",
    "        pid = r[\"policy_item_id\"]\n",
    "        if (pid not in best_by_ent) or (r[\"score\"] > best_by_ent[pid][\"score\"]):\n",
    "            best_by_ent[pid] = r\n",
    "\n",
    "    out = sorted(best_by_ent.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "    return out[:top_k]\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Search (multivector)\n",
    "# ===============================================================\n",
    "def section_boost(section: str) -> float:\n",
    "    if section in SECTION_BOOST:\n",
    "        return SECTION_BOOST[section]\n",
    "    if section.startswith(\"COMMON_\"):\n",
    "        return 1.00\n",
    "    return 1.00\n",
    "\n",
    "def search_multivector(query, model, emb, index, df_lookup, top_k=TOP_K_DEFAULT):\n",
    "    q_emb = model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    # ANN / sims across ALL name texts\n",
    "    if FAISS_AVAILABLE and index is not None:\n",
    "        scores, idx = index.search(np.array(q_emb, dtype=\"float32\"), min(RECALL_K, len(df_lookup)))\n",
    "        idx, scores = idx[0], scores[0]\n",
    "    else:\n",
    "        sims = cosine_similarity(q_emb, emb)[0]\n",
    "        idx = np.argsort(sims)[::-1][:min(RECALL_K, len(df_lookup))]\n",
    "        scores = sims[idx]\n",
    "\n",
    "    # Row-level blend + per-entity aggregation (max)\n",
    "    q_norm = normalize_query_lex(query)\n",
    "    hits = []\n",
    "    for i, s in zip(idx, scores):\n",
    "        row = df_lookup.iloc[i]\n",
    "        text = str(row.get(\"text\", \"\"))\n",
    "        canon = str(row.get(\"canonical\", \"\"))\n",
    "        section = str(row.get(\"section\", \"\"))\n",
    "        lang = str(row.get(\"language\", \"\"))\n",
    "\n",
    "        lex1 = fuzz.token_set_ratio(q_norm, normalize_query_lex(text)) / 100\n",
    "        lex2 = fuzz.token_set_ratio(q_norm, normalize_query_lex(canon)) / 100 if canon else 0.0\n",
    "        lex  = max(lex1, lex2)\n",
    "\n",
    "        # Section boost helps e.g., CANON_LAT/COMMON_XX\n",
    "        boosted_sem = float(s) * section_boost(section)\n",
    "        final = ALPHA_SEM * boosted_sem + (1 - ALPHA_SEM) * lex\n",
    "\n",
    "        hits.append({\n",
    "            \"policy_item_id\": str(row[\"policy_item_id\"]),\n",
    "            \"canonical\": canon,\n",
    "            \"best_text\": text,\n",
    "            \"section\": section,\n",
    "            \"language\": lang,\n",
    "            \"semantic\": round(float(s), 3),\n",
    "            \"lexical\": round(lex, 3),\n",
    "            \"score\": round(final, 3),\n",
    "        })\n",
    "\n",
    "    # Aggregate by entity (keep best scoring row per entity)\n",
    "    best_by_ent = {}\n",
    "    for h in hits:\n",
    "        pid = h[\"policy_item_id\"]\n",
    "        if (pid not in best_by_ent) or (h[\"score\"] > best_by_ent[pid][\"score\"]):\n",
    "            best_by_ent[pid] = h\n",
    "\n",
    "    out = sorted(best_by_ent.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "    # Confidence filter\n",
    "    out = [r for r in out if r[\"score\"] >= MIN_CONFIDENCE]\n",
    "    return out[:top_k]\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CLI\n",
    "# ===============================================================\n",
    "def main():\n",
    "    print(f\"üß† Ingredient Search ‚Äî Mode: {MODE}\")\n",
    "\n",
    "    if MODE == \"cards\":\n",
    "        model, emb, index, df_lookup = build_or_load_cards_index()\n",
    "        search_fn = search_cards\n",
    "    elif MODE == \"multivector\":\n",
    "        model, emb, index, df_lookup = build_or_load_multivector_index()\n",
    "        search_fn = search_multivector\n",
    "    else:\n",
    "        raise ValueError(\"MODE must be 'cards' or 'multivector'\")\n",
    "\n",
    "    print(\"\\n‚úÖ Ready. Type any ingredient name (or 'exit' to quit).\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nüîç Enter ingredient name: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            query = \"exit\"\n",
    "\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"üëã Exiting. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        results = search_fn(query, model, emb, index, df_lookup, top_k=TOP_K_DEFAULT)\n",
    "\n",
    "        print(f\"\\nResults for '{query}':\")\n",
    "        print(\"=\" * 70)\n",
    "        if not results:\n",
    "            print(\"No confident match (below threshold). Try another term.\")\n",
    "        else:\n",
    "            for r in results:\n",
    "                print(\n",
    "                    f\"Entity: {r['canonical']} (ID: {r['policy_item_id']})\\n\"\n",
    "                    f\"  Best match text: {r['best_text']}\\n\"\n",
    "                    f\"  Section/Lang: {r['section']} / {r['language']}\\n\"\n",
    "                    f\"  Scores ‚Üí semantic: {r['semantic']}, lexical: {r['lexical']}, final: {r['score']}\\n\"\n",
    "                    f\"{'-'*70}\"\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
